
@misc{lee_enhancing_2025,
	title = {Enhancing {Listened} {Speech} {Decoding} from {EEG} via {Parallel} {Phoneme} {Sequence} {Prediction}},
	url = {http://arxiv.org/abs/2501.04844},
	doi = {10.48550/arXiv.2501.04844},
	abstract = {Brain-computer interfaces (BCI) offer numerous human-centered application possibilities, particularly affecting people with neurological disorders. Text or speech decoding from brain activities is a relevant domain that could augment the quality of life for people with impaired speech perception. We propose a novel approach to enhance listened speech decoding from electroencephalography (EEG) signals by utilizing an auxiliary phoneme predictor that simultaneously decodes textual phoneme sequences. The proposed model architecture consists of three main parts: EEG module, speech module, and phoneme predictor. The EEG module learns to properly represent EEG signals into EEG embeddings. The speech module generates speech waveforms from the EEG embeddings. The phoneme predictor outputs the decoded phoneme sequences in text modality. Our proposed approach allows users to obtain decoded listened speech from EEG signals in both modalities (speech waveforms and textual phoneme sequences) simultaneously, eliminating the need for a concatenated sequential pipeline for each modality. The proposed approach also outperforms previous methods in both modalities. The source code and speech samples are publicly available.},
	urldate = {2025-10-05},
	publisher = {arXiv},
	author = {Lee, Jihwan and Feng, Tiantian and Kommineni, Aditya and Kadiri, Sudarsana Reddy and Narayanan, Shrikanth},
	month = jan,
	year = {2025},
	note = {arXiv:2501.04844 [eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing, Electrical Engineering and Systems Science - Signal Processing},
	file = {Preprint PDF:C\:\\Users\\name\\Zotero\\storage\\PV5P66G4\\Lee et al. - 2025 - Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction.pdf:application/pdf;Snapshot:C\:\\Users\\name\\Zotero\\storage\\V3FGCCEF\\2501.html:text/html},
}

@misc{papastratis_speech_2021,
	title = {Speech {Recognition}: a review of the different deep learning approaches},
	shorttitle = {Speech {Recognition}},
	url = {https://theaisummer.com/speech-recognition/},
	abstract = {Explore the most popular deep learning architecture to perform automatic speech recognition (ASR). From recurrent neural networks to convolutional and transformers.},
	language = {en},
	urldate = {2025-10-05},
	journal = {AI Summer},
	author = {Papastratis, Ilias},
	month = jul,
	year = {2021},
	file = {Snapshot:C\:\\Users\\name\\Zotero\\storage\\PBABBCMT\\speech-recognition.html:text/html},
}

@misc{lee_eeg-transformer_2021,
	title = {{EEG}-{Transformer}: {Self}-attention from {Transformer} {Architecture} for {Decoding} {EEG} of {Imagined} {Speech}},
	shorttitle = {{EEG}-{Transformer}},
	url = {http://arxiv.org/abs/2112.09239},
	doi = {10.48550/arXiv.2112.09239},
	abstract = {Transformers are groundbreaking architectures that have changed a flow of deep learning, and many high-performance models are developing based on transformer architectures. Transformers implemented only with attention with encoder-decoder structure following seq2seq without using RNN, but had better performance than RNN. Herein, we investigate the decoding technique for electroencephalography (EEG) composed of self-attention module from transformer architecture during imagined speech and overt speech. We performed classification of nine subjects using convolutional neural network based on EEGNet that captures temporal-spectral-spatial features from EEG of imagined speech and overt speech. Furthermore, we applied the self-attention module to decoding EEG to improve the performance and lower the number of parameters. Our results demonstrate the possibility of decoding brain activities of imagined speech and overt speech using attention modules. Also, only single channel EEG or ear-EEG can be used to decode the imagined speech for practical BCIs.},
	urldate = {2025-10-05},
	publisher = {arXiv},
	author = {Lee, Young-Eun and Lee, Seo-Hyun},
	month = dec,
	year = {2021},
	note = {arXiv:2112.09239 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Preprint PDF:C\:\\Users\\name\\Zotero\\storage\\NDWVST5J\\Lee and Lee - 2021 - EEG-Transformer Self-attention from Transformer Architecture for Decoding EEG of Imagined Speech.pdf:application/pdf;Snapshot:C\:\\Users\\name\\Zotero\\storage\\CA9R2ISD\\2112.html:text/html},
}

@article{card_accurate_2024,
	title = {An accurate and rapidly calibrating speech neuroprosthesis},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC11030484/},
	doi = {10.1101/2023.12.26.23300110},
	abstract = {Brain-computer interfaces can enable rapid, intuitive communication for people with paralysis by transforming the cortical activity associated with attempted speech into text on a computer screen. Despite recent advances, communication with brain-computer interfaces has been restricted by extensive training data requirements and inaccurate word output. A man in his 40’s with ALS with tetraparesis and severe dysarthria (ALSFRS-R = 23) was enrolled into the BrainGate2 clinical trial. He underwent surgical implantation of four microelectrode arrays into his left precentral gyrus, which recorded neural activity from 256 intracortical electrodes. We report a speech neuroprosthesis that decoded his neural activity as he attempted to speak in both prompted and unstructured conversational settings. Decoded words were displayed on a screen, then vocalized using text-to-speech software designed to sound like his pre-ALS voice. On the first day of system use, following 30 minutes of attempted speech training data, the neuroprosthesis achieved 99.6\% accuracy with a 50-word vocabulary. On the second day, the size of the possible output vocabulary increased to 125,000 words, and, after 1.4 additional hours of training data, the neuroprosthesis achieved 90.2\% accuracy. With further training data, the neuroprosthesis sustained 97.5\% accuracy beyond eight months after surgical implantation. The participant has used the neuroprosthesis to communicate in self-paced conversations for over 248 hours. In an individual with ALS and severe dysarthria, an intracortical speech neuroprosthesis reached a level of performance suitable to restore naturalistic communication after a brief training period.},
	urldate = {2025-10-05},
	journal = {medRxiv},
	author = {Card, Nicholas S. and Wairagkar, Maitreyee and Iacobacci, Carrina and Hou, Xianda and Singer-Clark, Tyler and Willett, Francis R. and Kunz, Erin M. and Fan, Chaofei and Nia, Maryam Vahdati and Deo, Darrel R. and Srinivasan, Aparna and Choi, Eun Young and Glasser, Matthew F. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shahlaie, Kiarash and Brandman, David M. and Stavisky, Sergey D.},
	month = apr,
	year = {2024},
	pmid = {38645254},
	pmcid = {PMC11030484},
	pages = {2023.12.26.23300110},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\AP8AWT3I\\Card et al. - 2024 - An accurate and rapidly calibrating speech neuroprosthesis.pdf:application/pdf},
}

@misc{noauthor_word_2025,
	title = {Word error rate},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Word_error_rate&oldid=1281062858},
	abstract = {Word error rate (WER) is a common metric of the performance of a speech recognition or machine translation system. The WER metric typically ranges from 0 to 1, where 0 indicates that the compared pieces of text are exactly identical, and 1 (or larger) indicates that they are completely different with no similarity. This way, a WER of 0.8 means that there is an 80\% error rate for compared sentences.
The general difficulty of measuring performance lies in the fact that the recognized word sequence can have a different length from the reference word sequence (supposedly the correct one). The WER is derived from the Levenshtein distance, working at the word level instead of the phoneme level. The WER is a valuable tool for comparing different systems as well as for evaluating improvements within one system. This kind of measurement, however, provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.
This problem is solved by first aligning the recognized word sequence with the reference (spoken) word sequence using dynamic string alignment. Examination of this issue is seen through a theory called the power law that states the correlation between perplexity and word error rate.
Word error rate can then be computed as:

  
    
      
        
          
            W
            E
            R
          
        
        =
        
          
            
              S
              +
              D
              +
              I
            
            N
          
        
        =
        
          
            
              S
              +
              D
              +
              I
            
            
              S
              +
              D
              +
              C
            
          
        
      
    
    \{{\textbackslash}displaystyle \{{\textbackslash}mathit \{WER\}\}=\{{\textbackslash}frac \{S+D+I\}\{N\}\}=\{{\textbackslash}frac \{S+D+I\}\{S+D+C\}\}\}
  

where

S is the number of substitutions,
D is the number of deletions,
I is the number of insertions,
C is the number of correct words,
N is the number of words in the reference (N=S+D+C)
The intuition behind 'deletion' and 'insertion' is how to get from the reference to the hypothesis. So if we have the reference "This is wikipedia" and hypothesis "This \_ wikipedia", we call it a deletion.
Note that since N is the number of words in the reference, the word error rate can be larger than 1.0, namely if the number of insertions I is larger than the number of correct words C.
When reporting the performance of a speech recognition system, sometimes word accuracy (WAcc) is used instead:

  
    
      
        
          
            W
            A
            c
            c
          
        
        =
        1
        −
        
          
            W
            E
            R
          
        
        =
        
          
            
              N
              −
              S
              −
              D
              −
              I
            
            N
          
        
        =
        
          
            
              C
              −
              I
            
            N
          
        
      
    
    \{{\textbackslash}displaystyle \{{\textbackslash}mathit \{WAcc\}\}=1-\{{\textbackslash}mathit \{WER\}\}=\{{\textbackslash}frac \{N-S-D-I\}\{N\}\}=\{{\textbackslash}frac \{C-I\}\{N\}\}\}
  

Since the WER can be larger than 1.0, the word accuracy can be smaller than 0.0.},
	language = {en},
	urldate = {2025-10-05},
	journal = {Wikipedia},
	month = mar,
	year = {2025},
	note = {Page Version ID: 1281062858},
	file = {Snapshot:C\:\\Users\\name\\Zotero\\storage\\EJHU3RUY\\index.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-10-05},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\name\\Zotero\\storage\\V6BPHHQA\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@article{willett_high-performance_2023,
	title = {A high-performance speech neuroprosthesis},
	volume = {620},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06377-x},
	doi = {10.1038/s41586-023-06377-x},
	abstract = {Speech brain–computer interfaces (BCIs) have the potential to restore rapid communication to people with paralysis by decoding neural activity evoked by attempted speech into text1,2 or sound3,4. Early demonstrations, although promising, have not yet achieved accuracies sufficiently high for communication of unconstrained sentences from a large vocabulary1–7. Here we demonstrate a speech-to-text BCI that records spiking activity from intracortical microelectrode arrays. Enabled by these high-resolution recordings, our study participant—who can no longer speak intelligibly owing to amyotrophic lateral sclerosis—achieved a 9.1\% word error rate on a 50-word vocabulary (2.7 times fewer errors than the previous state-of-the-art speech BCI2) and a 23.8\% word error rate on a 125,000-word vocabulary (the first successful demonstration, to our knowledge, of large-vocabulary decoding). Our participant’s attempted speech was decoded  at 62 words per minute, which is 3.4 times as fast as the previous record8 and begins to approach the speed of natural conversation (160 words per minute9). Finally, we highlight two aspects of the neural code for speech that are encouraging for speech BCIs: spatially intermixed tuning to speech articulators that makes accurate decoding possible from only a small region of cortex, and a detailed articulatory representation of phonemes that persists years after paralysis. These results show a feasible path forward for restoring rapid communication to people with paralysis who can no longer speak.},
	language = {en},
	number = {7976},
	urldate = {2025-10-05},
	journal = {Nature},
	author = {Willett, Francis R. and Kunz, Erin M. and Fan, Chaofei and Avansino, Donald T. and Wilson, Guy H. and Choi, Eun Young and Kamdar, Foram and Glasser, Matthew F. and Hochberg, Leigh R. and Druckmann, Shaul and Shenoy, Krishna V. and Henderson, Jaimie M.},
	month = aug,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Brain–machine interface, Neural decoding},
	pages = {1031--1036},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\987TXBWW\\Willett et al. - 2023 - A high-performance speech neuroprosthesis.pdf:application/pdf},
}

@article{card_accurate_2024-1,
	title = {An {Accurate} and {Rapidly} {Calibrating} {Speech} {Neuroprosthesis}},
	volume = {391},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMoa2314132},
	doi = {10.1056/NEJMoa2314132},
	abstract = {In a man with impaired speech from amyotrophic lateral sclerosis, an intracortical speech neuroprosthesis achieved more than 97\% accuracy in decoding his intended speech and making it audible in his natural voice.},
	number = {7},
	urldate = {2025-10-05},
	journal = {New England Journal of Medicine},
	author = {Card, Nicholas S. and Wairagkar, Maitreyee and Iacobacci, Carrina and Hou, Xianda and Singer-Clark, Tyler and Willett, Francis R. and Kunz, Erin M. and Fan, Chaofei and Nia, Maryam Vahdati and Deo, Darrel R. and Srinivasan, Aparna and Choi, Eun Young and Glasser, Matthew F. and Hochberg, Leigh R. and Henderson, Jaimie M. and Shahlaie, Kiarash and Stavisky, Sergey D. and Brandman, David M.},
	month = aug,
	year = {2024},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJMoa2314132},
	pages = {609--618},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\VDQCTQ4V\\Card et al. - 2024 - An Accurate and Rapidly Calibrating Speech Neuroprosthesis.pdf:application/pdf},
}

@article{chen_neural_2024,
	title = {A neural speech decoding framework leveraging deep learning and speech synthesis},
	volume = {6},
	copyright = {2024 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-024-00824-8},
	doi = {10.1038/s42256-024-00824-8},
	abstract = {Decoding human speech from neural signals is essential for brain–computer interface (BCI) technologies that aim to restore speech in populations with neurological deficits. However, it remains a highly challenging task, compounded by the scarce availability of neural signals with corresponding speech, data complexity and high dimensionality. Here we present a novel deep learning-based neural speech decoding framework that includes an ECoG decoder that translates electrocorticographic (ECoG) signals from the cortex into interpretable speech parameters and a novel differentiable speech synthesizer that maps speech parameters to spectrograms. We have developed a companion speech-to-speech auto-encoder consisting of a speech encoder and the same speech synthesizer to generate reference speech parameters to facilitate the ECoG decoder training. This framework generates natural-sounding speech and is highly reproducible across a cohort of 48 participants. Our experimental results show that our models can decode speech with high correlation, even when limited to only causal operations, which is necessary for adoption by real-time neural prostheses. Finally, we successfully decode speech in participants with either left or right hemisphere coverage, which could lead to speech prostheses in patients with deficits resulting from left hemisphere damage.},
	language = {en},
	number = {4},
	urldate = {2025-10-05},
	journal = {Nature Machine Intelligence},
	author = {Chen, Xupeng and Wang, Ran and Khalilian-Gourtani, Amirhossein and Yu, Leyao and Dugan, Patricia and Friedman, Daniel and Doyle, Werner and Devinsky, Orrin and Wang, Yao and Flinker, Adeen},
	month = apr,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cortex, Neural decoding},
	pages = {467--480},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\Y4IVS4PM\\Chen et al. - 2024 - A neural speech decoding framework leveraging deep learning and speech synthesis.pdf:application/pdf},
}

@misc{sun_brain2char_2019,
	title = {{Brain2Char}: {A} {Deep} {Architecture} for {Decoding} {Text} from {Brain} {Recordings}},
	shorttitle = {{Brain2Char}},
	url = {http://arxiv.org/abs/1909.01401},
	doi = {10.48550/arXiv.1909.01401},
	abstract = {Decoding language representations directly from the brain can enable new Brain-Computer Interfaces (BCI) for high bandwidth human-human and human-machine communication. Clinically, such technologies can restore communication in people with neurological conditions affecting their ability to speak. In this study, we propose a novel deep network architecture Brain2Char, for directly decoding text (specifically character sequences) from direct brain recordings (called Electrocorticography, ECoG). Brain2Char framework combines state-of-the-art deep learning modules --- 3D Inception layers for multiband spatiotemporal feature extraction from neural data and bidirectional recurrent layers, dilated convolution layers followed by language model weighted beam search to decode character sequences, optimizing a connectionist temporal classification (CTC) loss. Additionally, given the highly non-linear transformations that underlie the conversion of cortical function to character sequences, we perform regularizations on the network's latent representations motivated by insights into cortical encoding of speech production and artifactual aspects specific to ECoG data acquisition. To do this, we impose auxiliary losses on latent representations for articulatory movements, speech acoustics and session specific non-linearities. In 3 participants tested here, Brain2Char achieves 10.6{\textbackslash}\%, 8.5{\textbackslash}\% and 7.0{\textbackslash}\% Word Error Rates (WER) respectively on vocabulary sizes ranging from 1200 to 1900 words. Brain2Char also performs well when 2 participants silently mimed sentences. These results set a new state-of-the-art on decoding text from brain and demonstrate the potential of Brain2Char as a high-performance communication BCI.},
	urldate = {2025-10-05},
	publisher = {arXiv},
	author = {Sun, Pengfei and Anumanchipalli, Gopala K. and Chang, Edward F.},
	month = sep,
	year = {2019},
	note = {arXiv:1909.01401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\name\\Zotero\\storage\\L6AIHBEU\\Sun et al. - 2019 - Brain2Char A Deep Architecture for Decoding Text from Brain Recordings.pdf:application/pdf},
}

@article{stavisky_restoring_2025,
	title = {Restoring {Speech} {Using} {Brain}–{Computer} {Interfaces}},
	volume = {27},
	issn = {1523-9829, 1545-4274},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-bioeng-110122-012818},
	doi = {10.1146/annurev-bioeng-110122-012818},
	abstract = {People who have lost the ability to speak due to neurological injuries would greatly benefit from assistive technology that provides a fast, intuitive, and naturalistic means of communication. This need can be met with brain–computer interfaces (BCIs): medical devices that bypass injured parts of the nervous system and directly transform neural activity into outputs such as text or sound. BCIs for restoring movement and typing have progressed rapidly in recent clinical trials; speech BCIs are the next frontier. This review covers the clinical need for speech BCIs, surveys foundational studies that point to where and how speech can be decoded in the brain, describes recent progress in both discrete and continuous speech decoding and closed-loop speech BCIs, provides metrics for assessing these systems’ performance, and highlights key remaining challenges on the road toward clinically useful speech neuroprostheses.},
	language = {en},
	number = {Volume 27, 2025},
	urldate = {2025-10-05},
	journal = {Annual Review of Biomedical Engineering},
	author = {Stavisky, Sergey D.},
	month = may,
	year = {2025},
	note = {Publisher: Annual Reviews},
	pages = {29--54},
}

@inproceedings{feng_towards_2024,
	title = {Towards an {End}-to-{End} {Framework} for {Invasive} {Brain} {Signal} {Decoding} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2406.11568},
	doi = {10.21437/Interspeech.2024-382},
	abstract = {In this paper, we introduce a groundbreaking end-to-end (E2E) framework for decoding invasive brain signals, marking a significant advancement in the field of speech neuroprosthesis. Our methodology leverages the comprehensive reasoning abilities of large language models (LLMs) to facilitate direct decoding. By fully integrating LLMs, we achieve results comparable to the state-of-the-art cascade models. Our findings underscore the immense potential of E2E frameworks in speech neuroprosthesis, particularly as the technology behind brain-computer interfaces (BCIs) and the availability of relevant datasets continue to evolve. This work not only showcases the efficacy of combining LLMs with E2E decoding for enhancing speech neuroprosthesis but also sets a new direction for future research in BCI applications, underscoring the impact of LLMs in decoding complex neural signals for communication restoration. Code will be made available at https://github.com/FsFrancis15/BrainLLM.},
	urldate = {2025-10-05},
	booktitle = {Interspeech 2024},
	author = {Feng, Sheng and Liu, Heyang and Wang, Yu and Wang, Yanfeng},
	month = sep,
	year = {2024},
	note = {arXiv:2406.11568 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Quantitative Biology - Neurons and Cognition},
	pages = {1495--1499},
	file = {Preprint PDF:C\:\\Users\\name\\Zotero\\storage\\M9T2F4ES\\Feng et al. - 2024 - Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models.pdf:application/pdf},
}

@article{kunz_inner_2025,
	title = {Inner speech in motor cortex and implications for speech neuroprostheses},
	volume = {188},
	issn = {0092-8674, 1097-4172},
	url = {https://www.cell.com/cell/abstract/S0092-8674(25)00681-6},
	doi = {10.1016/j.cell.2025.06.015},
	language = {English},
	number = {17},
	urldate = {2025-10-05},
	journal = {Cell},
	author = {Kunz, Erin M. and Krasa, Benyamin Abramovich and Kamdar, Foram and Avansino, Donald T. and Hahn, Nick and Yoon, Seonghyun and Singh, Akansha and Nason-Tomaszewski, Samuel R. and Card, Nicholas S. and Jude, Justin J. and Jacques, Brandon G. and Bechefsky, Payton H. and Iacobacci, Carrina and Hochberg, Leigh R. and Rubin, Daniel B. and Williams, Ziv M. and Brandman, David M. and Stavisky, Sergey D. and AuYong, Nicholas and Pandarinath, Chethan and Druckmann, Shaul and Henderson, Jaimie M. and Willett, Francis R.},
	month = aug,
	year = {2025},
	pmid = {40816265},
	note = {Publisher: Elsevier},
	keywords = {brain-computer interface, covert speech, inner speech, motor cortex, speech neuroprosthesis},
	pages = {4658--4673.e17},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\7SRHMQVR\\Kunz et al. - 2025 - Inner speech in motor cortex and implications for speech neuroprostheses.pdf:application/pdf},
}

@article{proix_imagined_2022,
	title = {Imagined speech can be decoded from low- and cross-frequency intracranial {EEG} features},
	volume = {13},
	issn = {2041-1723},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC8748882/},
	doi = {10.1038/s41467-021-27725-3},
	abstract = {Reconstructing intended speech from neural activity using brain-computer interfaces holds great promises for people with severe speech production deficits. While decoding overt speech has progressed, decoding imagined speech has met limited success, mainly because the associated neural signals are weak and variable compared to overt speech, hence difficult to decode by learning algorithms. We obtained three electrocorticography datasets from 13 patients, with electrodes implanted for epilepsy evaluation, who performed overt and imagined speech production tasks. Based on recent theories of speech neural processing, we extracted consistent and specific neural features usable for future brain computer interfaces, and assessed their performance to discriminate speech items in articulatory, phonetic, and vocalic representation spaces. While high-frequency activity provided the best signal for overt speech, both low- and higher-frequency power and local cross-frequency contributed to imagined speech decoding, in particular in phonetic and vocalic, i.e. perceptual, spaces. These findings show that low-frequency power and cross-frequency dynamics contain key information for imagined speech decoding., Reconstructing imagined speech from neural activity holds great promises for people with severe speech production deficits. Here, the authors demonstrate using human intracranial recordings that both low- and higher-frequency power and local cross-frequency contribute to imagined speech decoding.},
	urldate = {2025-10-05},
	journal = {Nature Communications},
	author = {Proix, Timothée and Delgado Saa, Jaime and Christen, Andy and Martin, Stephanie and Pasley, Brian N. and Knight, Robert T. and Tian, Xing and Poeppel, David and Doyle, Werner K. and Devinsky, Orrin and Arnal, Luc H. and Mégevand, Pierre and Giraud, Anne-Lise},
	month = jan,
	year = {2022},
	pmid = {35013268},
	pmcid = {PMC8748882},
	pages = {48},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\C6787VTH\\Proix et al. - 2022 - Imagined speech can be decoded from low- and cross-frequency intracranial EEG features.pdf:application/pdf},
}

@article{dash_neural_2024,
	title = {Neural {Decoding} of {Spontaneous} {Overt} and {Intended} {Speech}},
	volume = {67},
	issn = {1558-9102},
	doi = {10.1044/2024_JSLHR-24-00046},
	abstract = {PURPOSE: The aim of this study was to decode intended and overt speech from neuromagnetic signals while the participants performed spontaneous overt speech tasks without cues or prompts (stimuli).
METHOD: Magnetoencephalography (MEG), a noninvasive neuroimaging technique, was used to collect neural signals from seven healthy adult English speakers performing spontaneous, overt speech tasks. The participants randomly spoke the words yes or no at a self-paced rate without cues. Two machine learning models, namely, linear discriminant analysis (LDA) and one-dimensional convolutional neural network (1D CNN), were employed to classify the two words from the recorded MEG signals.
RESULTS: LDA and 1D CNN achieved average decoding accuracies of 79.02\% and 90.40\%, respectively, in decoding overt speech, significantly surpassing the chance level (50\%). The accuracy for decoding intended speech was 67.19\% using 1D CNN.
CONCLUSIONS: This study showcases the possibility of decoding spontaneous overt and intended speech directly from neural signals in the absence of perceptual interference. We believe that these findings make a steady step toward the future spontaneous speech-based brain-computer interface.},
	language = {eng},
	number = {11},
	journal = {Journal of speech, language, and hearing research: JSLHR},
	author = {Dash, Debadatta and Ferrari, Paul and Wang, Jun},
	month = nov,
	year = {2024},
	pmid = {39106199},
	pmcid = {PMC12379578},
	keywords = {Adult, Brain, Brain-Computer Interfaces, Discriminant Analysis, Female, Humans, Machine Learning, Magnetoencephalography, Male, Neural Networks, Computer, Speech, Speech Perception, Young Adult},
	pages = {4216--4225},
	file = {Full Text PDF:C\:\\Users\\name\\Zotero\\storage\\VEW7ATV2\\Dash et al. - 2024 - Neural Decoding of Spontaneous Overt and Intended Speech.pdf:application/pdf},
}
